{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "model_name = \"your-model-name\"  # Replace with the model name or path to your custom model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with memory-efficient settings\n",
    "try:\n",
    "    from bitsandbytes import quantization\n",
    "    \n",
    "    # Load model with 4-bit quantization for memory efficiency\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,  \n",
    "        quantization_config=quantization.Linear8bitLtConfig(), \n",
    "        device_map=\"auto\"   \n",
    "    ).to(device)\n",
    "except ImportError as e:\n",
    "    # Fallback if bitsandbytes is not available or GPU is not supported\n",
    "    print(\"Error: bitsandbytes module import failed or missing GPU support.\")\n",
    "    print(e)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "dataset = load_dataset(\"your-dataset-name\")  # Replace with the dataset name or path\n",
    "\n",
    "# Define a function to tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence1\"],\n",
    "        examples[\"sentence2\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# Optionally remove unnecessary columns if needed\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"X\", \"Y\", \"Z\"])\n",
    "tokenized_datasets.set_format(\"torch\")  # Convert dataset to PyTorch tensors\n",
    "\n",
    "# Configure LoRA (Low-Rank Adaptation) with PEFT (Parameter-Efficient Fine-Tuning)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"classifier\"], \n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model for training\n",
    "model = BetterTransformer.transform(model)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3, \n",
    "    save_steps=10_000, \n",
    "    save_total_limit=2, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1_000, \n",
    "    logging_dir=\"./logs\",  \n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "# Create a Trainer instance with the model and training arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"]   # Validation dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert the model to its original state (undo BetterTransformer optimizations)\n",
    "model = BetterTransformer.reverse(model)\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./name\")  # Save the model to the specified directory\n",
    "tokenizer.save_pretrained(\"./name\")  # Save the tokenizer to the specified directory\n",
    "\n",
    "print(\"Training completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
